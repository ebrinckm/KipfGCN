{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<table style=\"border: 2px solid white;\">\n<tr>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Client</h3>\n<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n  <li><b>Scheduler: </b>tcp://127.0.0.1:56829</li>\n  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n</ul>\n</td>\n<td style=\"vertical-align: top; border: 0px solid white\">\n<h3 style=\"text-align: left;\">Cluster</h3>\n<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n  <li><b>Workers: </b>4</li>\n  <li><b>Cores: </b>8</li>\n  <li><b>Memory: </b>8.44 GB</li>\n</ul>\n</td>\n</tr>\n</table>",
      "text/plain": "<Client: 'tcp://127.0.0.1:56829' processes=4 threads=8, memory=8.44 GB>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\r\n",
    "\r\n",
    "cluster = LocalCluster()\r\n",
    "client = Client(cluster)\r\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\r\n",
    "import datetime\r\n",
    "import glob\r\n",
    "\r\n",
    "v = lambda x: (int(x)/1e18)\r\n",
    "g = lambda x: (int(x)/1e9)\r\n",
    "t = lambda x: datetime.datetime.fromtimestamp(int(x))\r\n",
    "\r\n",
    "# Concating all of my csv files together:\r\n",
    "# https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe\r\n",
    "\r\n",
    "files = ['C:/Data_Files/transactions/eth_txs-201901_0.csv','C:/Data_Files/transactions/eth_txs-201901_1.csv','C:/Data_Files/transactions/eth_txs-201901_2.csv']\r\n",
    "#      'C:/Data_Files/transactions/eth_txs-201901_3.csv','C:/Data_Files/transactions/eth_txs-201901_4.csv','C:/Data_Files/transactions/eth_txs-201901_5.csv',\r\n",
    "#      'C:/Data_Files/transactions/eth_txs-201901_6.csv','C:/Data_Files/transactions/eth_txs-201901_7.csv','C:/Data_Files/transactions/eth_txs-201901_8.csv']\r\n",
    "data = []\r\n",
    "for filename in files:\r\n",
    "    dt = dd.read_csv(filename,\r\n",
    "                usecols=['hash','nonce','block_hash','block_number','transaction_index','from_address','to_address','value','gas','gas_price','block_timestamp'],\r\n",
    "                converters={'value': v, \r\n",
    "                            'gas_price': g, \r\n",
    "                            'gas': v,\r\n",
    "                            'block_timestamp': t})\r\n",
    "    data.append(dt)\r\n",
    "\r\n",
    "df = dd.concat(data, axis=0, ignore_index=True)\r\n",
    "\r\n",
    "# distribute the partitions of the dataset across the cluster\r\n",
    "df = client.persist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate aggregation\r\n",
    "a=df.groupby(['from_address']).agg({\r\n",
    "    'hash': ['count'],\r\n",
    "    'value': ['min','max','mean','std']\r\n",
    "})\r\n",
    "a.columns = ['in_count', 'in_min', 'in_max', 'in_mean', 'in_std']\r\n",
    "a.index.name = 'address'\r\n",
    "\r\n",
    "b=df.groupby(['to_address']).agg({\r\n",
    "    'hash': ['count'],\r\n",
    "    'value': ['min','max','mean','std']\r\n",
    "})\r\n",
    "b.columns = ['out_count', 'out_min', 'out_max', 'out_mean', 'out_std']\r\n",
    "b.index.name = 'address'\r\n",
    "\r\n",
    "features = a.merge(b, how='outer', left_index=True, right_index=True)\r\n",
    "features.in_count = features.in_count.astype(float)\r\n",
    "features.out_count = features.out_count.astype(float)\r\n",
    "\r\n",
    "features = features.fillna(-1e-18)\r\n",
    "# persist the features in cluster memory\r\n",
    "features = client.persist(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an aggregated list of adjacencies - we're not interested in how many transactions, just whether the nodes are connected\r\n",
    "adjdf = df.groupby(['from_address','to_address']).agg({\r\n",
    "    'hash':'count'\r\n",
    "}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be optimized\r\n",
    "adjmap = {}\r\n",
    "for i, row in adjdf.reset_index(drop=False).drop('hash',axis=1).sort_values(['from_address']).iterrows():\r\n",
    "    if row.from_address not in adjmap:\r\n",
    "        adjmap[row.from_address] = []\r\n",
    "    else:\r\n",
    "        adjmap[row.from_address].append(row.to_address)\r\n",
    "    \r\n",
    "    if len(adjmap[row.from_address])==0:\r\n",
    "        adjmap[row.from_address] = [row.to_address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map of addresses (based on the features dataset)\r\n",
    "unique_addrs = features.index.compute().values\r\n",
    "addrmap = {addr: ix for ix, addr in list(enumerate(unique_addrs))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free some memory\r\n",
    "client.cancel(df)\r\n",
    "client.cancel(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addrmap 80.00009155273438 MB\n",
      "adjmap 40.000099182128906 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\r\n",
    "print('addrmap',(sys.getsizeof(addrmap)/1024)/1024, 'MB')\r\n",
    "print('adjmap',(sys.getsizeof(adjmap)/1024)/1024, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\r\n",
    "\r\n",
    "# make these dictionaries delayed, so dask can push them around\r\n",
    "addrmap_ = delayed(addrmap)\r\n",
    "adjmap_ = delayed(adjmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import wait\r\n",
    "\r\n",
    "# by suggestion of Matt Rocklin, submit the dicts to one of the workers, and then replicate it to the others\r\n",
    "# https://stackoverflow.com/questions/48299356/override-dask-scheduler-to-concurrently-load-data-on-multiple-workers/52069109#52069109\r\n",
    "d1_ = client.submit(addrmap_)\r\n",
    "d2_ = client.submit(adjmap_)\r\n",
    "wait([d1_,d2_])\r\n",
    "client.replicate([d1_, d2_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lookup function \r\n",
    "# note that it's not a delayed function, because it runs on each of the addresses\r\n",
    "def get_adj_nodes(addr, adjmap, addrmap):\r\n",
    "    try:\r\n",
    "        # list of nodes adjacent to addr\r\n",
    "        adj = adjmap[addr]\r\n",
    "    except KeyError:\r\n",
    "        adj = [addr]\r\n",
    "    \r\n",
    "    # index of addr\r\n",
    "    element_ix = addrmap[addr]\r\n",
    "    # index of its neighbors\r\n",
    "    neighbors_ix = [addrmap[a] for a in adj]\r\n",
    "    return [element_ix, neighbors_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\r\n",
    "\r\n",
    "# to avoid overloading the cluster, we scatter the list of addresses into 10 partitions to be processed\r\n",
    "unique_addrs = db.from_sequence(list(addrmap.keys()), npartitions=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we map the lookup to each of these addresses, passing the dict futures as arguments\r\n",
    "comps_ = unique_addrs.map(get_adj_nodes, adjmap=adjmap_, addrmap=addrmap_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yello\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\distributed\\worker.py:3460: UserWarning: Large object of size 111.86 MB detected in task graph: \n",
      "  ([['0x00000000060c32d93a35a13bed526f8cbb472edb', [ ... e27b26a56']]],)\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  % (format_bytes(len(b)), s)\n"
     ]
    }
   ],
   "source": [
    "# run the whole thing\r\n",
    "results = comps_.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462039\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\r\n",
    "print(type(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\r\n",
    "import collections\r\n",
    "\r\n",
    "graph = collections.defaultdict(list)\r\n",
    "\r\n",
    "for i in results:\r\n",
    "    graph[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1462039\n",
      "Saving graph...\n",
      "... graph saved.\n"
     ]
    }
   ],
   "source": [
    "print(len(graph)) \r\n",
    "print(\"Saving graph...\")\r\n",
    "pickle.dump(graph, open('ind.' + 'exchange' + '.graph', 'wb'))\r\n",
    "print(\"... graph saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "name": "python37364bitd9313a86d1f64986b97223912f69c594"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "d1d688591d79676ed7d41497198d1df1f5ead6c1271f3dc4dae81928b036766a"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}